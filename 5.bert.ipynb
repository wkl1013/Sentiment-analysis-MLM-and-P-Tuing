{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d35cc5e",
   "metadata": {
    "cell_id": 39
   },
   "source": [
    "### åŠ è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a6bee8",
   "metadata": {
    "cell_id": 1
   },
   "outputs": [],
   "source": [
    "from utils import load_corpus_bert\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_PATH = \"./data/weibo2018/train.txt\"\n",
    "TEST_PATH = \"./data/weibo2018/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8369026b",
   "metadata": {
    "cell_id": 3
   },
   "outputs": [],
   "source": [
    "# åˆ†åˆ«åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_data = load_corpus_bert(TRAIN_PATH)\n",
    "test_data = load_corpus_bert(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725a34f0",
   "metadata": {
    "cell_id": 4
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>â€œä¹¦ä¸­è‡ªæœ‰é»„é‡‘å±‹ï¼Œä¹¦ä¸­è‡ªæœ‰é¢œå¦‚ç‰â€ã€‚æ²¿ç€å²æœˆçš„é•¿æ²³è·‹æ¶‰ï¼Œæˆ–æ˜¯é£å…‰æ—–æ—ï¼Œæˆ–æ˜¯å§¹ç´«å«£çº¢ï¼Œä¸‡åƒ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>è¿™æ˜¯è‹±è¶…è¢«é»‘çš„æœ€æƒ¨çš„ä¸€æ¬¡[äºŒå“ˆ][äºŒå“ˆ]åå‡ å¹´æ¥ï¼Œä¸­å›½åªæœ‰å­™ç»§æµ·ï¼Œè‘£æ–¹å“ï¼Œéƒ‘æ™ºï¼Œæé“ç™»é™†è¿‡è‹±...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ä¸­å›½è¿œæ´‹æµ·è¿é›†å›¢å‰¯æ€»ç»ç†ä¿æ›¾æ¸¯4æœˆ21æ—¥åœ¨ ä¸Šè¡¨ç¤ºï¼Œä¸­å¤®ä¼ä¸šâ€œèµ°å‡ºå»â€æ˜¯è¦ç«™åœ¨æ›´é«˜çš„å¹³å°å‚...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>çœ‹ã€Šæµæ˜ŸèŠ±å›­ã€‹å…¶å®ä¹Ÿè¿˜å¥½å•¦ï¼Œç°åœ¨çš„è§‚å¿µä»¥åŠæ—¶å°šçœ¼å…‰éƒ½ä¸ä¸€æ ·äº†ï¼Œæˆ–è®¸åå‡ å¹´ä¹‹åçš„äººçœ‹æˆ‘ä»¬çš„ç°åœ¨...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>æ±‰æ­¦å¸çš„ç½ªå·±è¯çš„çœŸå®æ€§å°½ç®¡å­˜åœ¨ç€äº‰è®®ï¼Œç„¶è€Œâ€œè½®å°ç½ªå·±è¯â€ä½œä¸ºä¸­å›½å†å²ä¸Šç¬¬ä¸€ä»½çš‡å¸è‡ªæˆ‘æ‰¹è¯„çš„æ–‡...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>ç«è½¦ä¸Šç¢°è§è¿™ç§å åˆ«äººä½ç½®è¿˜ç†ç›´æ°”å£®çš„ç‹å…«è›‹çœŸçš„å¿ƒç´¯ï¼ŒæŠ¥äº†ä¹˜è­¦åŠå¤©ä¸è§äººï¼Œåªå¥½ç¥ç—…é­”æ—©æ—¥æˆ˜èƒœä¹‹...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>å€’éœ‰å‚¬çš„ï¼Œåä¸Šæ™šç‚¹ä¸€ä¸ªå¤šå°æ—¶çš„æ±½è½¦ğŸš—ï¼Œåœ¨é«˜é€Ÿä¸Šå¸æœºå”å”è¯´ä»–æ²¡å¬æ¸…æˆ‘è¯´è¯ï¼Œsoæˆ‘ä¸€è·¯è¶…æƒ³ä¸Šå•æ‰€...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>æ€¥è¯Šç¬¬ä¸€å¤©ä¸Šç­ï¼Œè¯´ä¸ä¸Šçš„å¿ƒç´¯ï¼Œè¿™æ¼«é•¿çš„ä¸¤ä¸ªæœˆå¦‚ä½•è¿‡å•Š[æ‚²ä¼¤][æ‚²ä¼¤][æ‚²ä¼¤]  \\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>æˆ‘æ¯ä¸ªæœˆä¾›ç€çˆ±å¥‡è‰ºï¼Œç½‘æ˜“äº‘ï¼Œå¿«è¿ï¼ŒèŠ’æœTVï¼ŒåŒ…å›¾ç½‘ï¼Œè¿™äº›éƒ½æ˜¯å¤§ä¼ä¸šå•Šï¼Œæˆ‘ä¹Ÿæ˜¯å¿ƒç´¯ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜±ğŸ˜±ğŸ˜±...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>å…¶å®è¿™ç§ä¸¤å®¶æ¯«æ— äº¤é›† å› ä¸ºä¸€ä¸ªç²‰ä¸å•æ–¹é¢æ’©éªšå°±æ’•æˆä¸€ç‰‡ä¸Šå‡æˆä¸¤å®¶æˆ˜ç«çš„æƒ…å†µçœŸçš„å¾ˆç…¤æ„æ€ è¦æ˜¯...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0       â€œä¹¦ä¸­è‡ªæœ‰é»„é‡‘å±‹ï¼Œä¹¦ä¸­è‡ªæœ‰é¢œå¦‚ç‰â€ã€‚æ²¿ç€å²æœˆçš„é•¿æ²³è·‹æ¶‰ï¼Œæˆ–æ˜¯é£å…‰æ—–æ—ï¼Œæˆ–æ˜¯å§¹ç´«å«£çº¢ï¼Œä¸‡åƒ...      1\n",
       "1     è¿™æ˜¯è‹±è¶…è¢«é»‘çš„æœ€æƒ¨çš„ä¸€æ¬¡[äºŒå“ˆ][äºŒå“ˆ]åå‡ å¹´æ¥ï¼Œä¸­å›½åªæœ‰å­™ç»§æµ·ï¼Œè‘£æ–¹å“ï¼Œéƒ‘æ™ºï¼Œæé“ç™»é™†è¿‡è‹±...      0\n",
       "2      ä¸­å›½è¿œæ´‹æµ·è¿é›†å›¢å‰¯æ€»ç»ç†ä¿æ›¾æ¸¯4æœˆ21æ—¥åœ¨ ä¸Šè¡¨ç¤ºï¼Œä¸­å¤®ä¼ä¸šâ€œèµ°å‡ºå»â€æ˜¯è¦ç«™åœ¨æ›´é«˜çš„å¹³å°å‚...      1\n",
       "3     çœ‹ã€Šæµæ˜ŸèŠ±å›­ã€‹å…¶å®ä¹Ÿè¿˜å¥½å•¦ï¼Œç°åœ¨çš„è§‚å¿µä»¥åŠæ—¶å°šçœ¼å…‰éƒ½ä¸ä¸€æ ·äº†ï¼Œæˆ–è®¸åå‡ å¹´ä¹‹åçš„äººçœ‹æˆ‘ä»¬çš„ç°åœ¨...      1\n",
       "4     æ±‰æ­¦å¸çš„ç½ªå·±è¯çš„çœŸå®æ€§å°½ç®¡å­˜åœ¨ç€äº‰è®®ï¼Œç„¶è€Œâ€œè½®å°ç½ªå·±è¯â€ä½œä¸ºä¸­å›½å†å²ä¸Šç¬¬ä¸€ä»½çš‡å¸è‡ªæˆ‘æ‰¹è¯„çš„æ–‡...      1\n",
       "...                                                 ...    ...\n",
       "9995  ç«è½¦ä¸Šç¢°è§è¿™ç§å åˆ«äººä½ç½®è¿˜ç†ç›´æ°”å£®çš„ç‹å…«è›‹çœŸçš„å¿ƒç´¯ï¼ŒæŠ¥äº†ä¹˜è­¦åŠå¤©ä¸è§äººï¼Œåªå¥½ç¥ç—…é­”æ—©æ—¥æˆ˜èƒœä¹‹...      0\n",
       "9996  å€’éœ‰å‚¬çš„ï¼Œåä¸Šæ™šç‚¹ä¸€ä¸ªå¤šå°æ—¶çš„æ±½è½¦ğŸš—ï¼Œåœ¨é«˜é€Ÿä¸Šå¸æœºå”å”è¯´ä»–æ²¡å¬æ¸…æˆ‘è¯´è¯ï¼Œsoæˆ‘ä¸€è·¯è¶…æƒ³ä¸Šå•æ‰€...      0\n",
       "9997         æ€¥è¯Šç¬¬ä¸€å¤©ä¸Šç­ï¼Œè¯´ä¸ä¸Šçš„å¿ƒç´¯ï¼Œè¿™æ¼«é•¿çš„ä¸¤ä¸ªæœˆå¦‚ä½•è¿‡å•Š[æ‚²ä¼¤][æ‚²ä¼¤][æ‚²ä¼¤]  \\n      0\n",
       "9998  æˆ‘æ¯ä¸ªæœˆä¾›ç€çˆ±å¥‡è‰ºï¼Œç½‘æ˜“äº‘ï¼Œå¿«è¿ï¼ŒèŠ’æœTVï¼ŒåŒ…å›¾ç½‘ï¼Œè¿™äº›éƒ½æ˜¯å¤§ä¼ä¸šå•Šï¼Œæˆ‘ä¹Ÿæ˜¯å¿ƒç´¯ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜±ğŸ˜±ğŸ˜±...      0\n",
       "9999  å…¶å®è¿™ç§ä¸¤å®¶æ¯«æ— äº¤é›† å› ä¸ºä¸€ä¸ªç²‰ä¸å•æ–¹é¢æ’©éªšå°±æ’•æˆä¸€ç‰‡ä¸Šå‡æˆä¸¤å®¶æˆ˜ç«çš„æƒ…å†µçœŸçš„å¾ˆç…¤æ„æ€ è¦æ˜¯...      0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame(train_data, columns=[\"text\", \"label\"])\n",
    "df_test = pd.DataFrame(test_data, columns=[\"text\", \"label\"])\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db7f2c",
   "metadata": {
    "cell_id": 41
   },
   "source": [
    "### åŠ è½½Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67761c0d",
   "metadata": {
    "cell_id": 5
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "MODEL_PATH = \"./model/chinese_wwm_pytorch\"     # ä¸‹è½½åœ°å€è§ https://github.com/ymcui/Chinese-BERT-wwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e96c02",
   "metadata": {
    "cell_id": 6
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_pytorch were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)   # åˆ†è¯å™¨\n",
    "bert = BertModel.from_pretrained(MODEL_PATH)            # æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4f205",
   "metadata": {
    "cell_id": 43
   },
   "source": [
    "### ç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fedd1dcc",
   "metadata": {
    "cell_id": 7
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c83f073",
   "metadata": {
    "cell_id": 8
   },
   "outputs": [],
   "source": [
    "# è¶…å‚æ•°\n",
    "learning_rate = 1e-3\n",
    "input_size = 768\n",
    "num_epoches = 10\n",
    "batch_size = 100\n",
    "decay_rate = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8a11e8",
   "metadata": {
    "cell_id": 9
   },
   "outputs": [],
   "source": [
    "# æ•°æ®é›†\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df[\"text\"].tolist()\n",
    "        self.label = df[\"label\"].tolist()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "# è®­ç»ƒé›†\n",
    "train_data = MyDataset(df_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# æµ‹è¯•é›†\n",
    "test_data = MyDataset(df_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69602a96",
   "metadata": {
    "cell_id": 10
   },
   "outputs": [],
   "source": [
    "# ç½‘ç»œç»“æ„\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "net = Net(input_size).to(device)\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f90ef00",
   "metadata": {
    "cell_id": 34
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# æµ‹è¯•é›†æ•ˆæœæ£€éªŒ\n",
    "def test():\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words, labels in test_loader:\n",
    "            tokens = tokenizer(words, padding=True)\n",
    "            input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "            attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "            last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "            bert_output = last_hidden_states[0][:, 0]\n",
    "            outputs = net(bert_output)          # å‰å‘ä¼ æ’­\n",
    "            outputs = outputs.view(-1)          # å°†è¾“å‡ºå±•å¹³\n",
    "            y_pred.append(outputs)\n",
    "            y_true.append(labels)\n",
    "\n",
    "    y_prob = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    y_pred = y_prob.clone()\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "    \n",
    "    y_true = y_true.cpu()\n",
    "    y_pred = y_pred.cpu()\n",
    "    y_prob = y_prob.cpu()\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print(\"å‡†ç¡®ç‡:\", metrics.accuracy_score(y_true, y_pred))\n",
    "    print(\"AUC:\", metrics.roc_auc_score(y_true, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fb295f9",
   "metadata": {
    "cell_id": 11
   },
   "outputs": [],
   "source": [
    "# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e483cb2c",
   "metadata": {
    "cell_id": 14,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, step:10, loss:0.6813191771507263\n",
      "epoch:1, step:20, loss:0.6367926001548767\n",
      "epoch:1, step:30, loss:0.5908938646316528\n",
      "epoch:1, step:40, loss:0.5482066869735718\n",
      "epoch:1, step:50, loss:0.5113250017166138\n",
      "epoch:1, step:60, loss:0.5121451616287231\n",
      "epoch:1, step:70, loss:0.5019151568412781\n",
      "epoch:1, step:80, loss:0.5041934251785278\n",
      "epoch:1, step:90, loss:0.49353915452957153\n",
      "epoch:1, step:100, loss:0.4792908728122711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75       155\n",
      "           1       0.90      0.86      0.88       345\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.81      0.83      0.82       500\n",
      "weighted avg       0.85      0.84      0.84       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.84\n",
      "AUC: 0.9012435717625058\n",
      "saved model:  ./model/bert_dnn_1.model\n",
      "epoch:2, step:10, loss:0.46031346917152405\n",
      "epoch:2, step:20, loss:0.46189332008361816\n",
      "epoch:2, step:30, loss:0.4660502076148987\n",
      "epoch:2, step:40, loss:0.4732876420021057\n",
      "epoch:2, step:50, loss:0.4567083418369293\n",
      "epoch:2, step:60, loss:0.45447808504104614\n",
      "epoch:2, step:70, loss:0.4462393820285797\n",
      "epoch:2, step:80, loss:0.4350458085536957\n",
      "epoch:2, step:90, loss:0.45661574602127075\n",
      "epoch:2, step:100, loss:0.43131086230278015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.77       155\n",
      "           1       0.92      0.85      0.88       345\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.82      0.84      0.83       500\n",
      "weighted avg       0.86      0.85      0.85       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.846\n",
      "AUC: 0.9128938756428238\n",
      "saved model:  ./model/bert_dnn_2.model\n",
      "epoch:3, step:10, loss:0.432222843170166\n",
      "epoch:3, step:20, loss:0.43521466851234436\n",
      "epoch:3, step:30, loss:0.4310072064399719\n",
      "epoch:3, step:40, loss:0.41495609283447266\n",
      "epoch:3, step:50, loss:0.4109768867492676\n",
      "epoch:3, step:60, loss:0.45129290223121643\n",
      "epoch:3, step:70, loss:0.43410035967826843\n",
      "epoch:3, step:80, loss:0.4282527565956116\n",
      "epoch:3, step:90, loss:0.43675899505615234\n",
      "epoch:3, step:100, loss:0.44812995195388794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.77       155\n",
      "           1       0.92      0.85      0.89       345\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.82      0.85      0.83       500\n",
      "weighted avg       0.86      0.85      0.85       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.848\n",
      "AUC: 0.9171575502571295\n",
      "saved model:  ./model/bert_dnn_3.model\n",
      "epoch:4, step:10, loss:0.43588048219680786\n",
      "epoch:4, step:20, loss:0.41449791193008423\n",
      "epoch:4, step:30, loss:0.42159825563430786\n",
      "epoch:4, step:40, loss:0.43740397691726685\n",
      "epoch:4, step:50, loss:0.4103790819644928\n",
      "epoch:4, step:60, loss:0.4272201955318451\n",
      "epoch:4, step:70, loss:0.401478111743927\n",
      "epoch:4, step:80, loss:0.428305447101593\n",
      "epoch:4, step:90, loss:0.4094158709049225\n",
      "epoch:4, step:100, loss:0.4253961741924286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78       155\n",
      "           1       0.91      0.88      0.90       345\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.83      0.85      0.84       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.858\n",
      "AUC: 0.9216830294530154\n",
      "saved model:  ./model/bert_dnn_4.model\n",
      "epoch:5, step:10, loss:0.39831170439720154\n",
      "epoch:5, step:20, loss:0.41463977098464966\n",
      "epoch:5, step:30, loss:0.41155534982681274\n",
      "epoch:5, step:40, loss:0.4231754243373871\n",
      "epoch:5, step:50, loss:0.4058705270290375\n",
      "epoch:5, step:60, loss:0.41910943388938904\n",
      "epoch:5, step:70, loss:0.41561374068260193\n",
      "epoch:5, step:80, loss:0.4222615659236908\n",
      "epoch:5, step:90, loss:0.3959978520870209\n",
      "epoch:5, step:100, loss:0.4395703375339508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78       155\n",
      "           1       0.91      0.88      0.90       345\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.83      0.85      0.84       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.86\n",
      "AUC: 0.9242262739597944\n",
      "saved model:  ./model/bert_dnn_5.model\n",
      "epoch:6, step:10, loss:0.3959680199623108\n",
      "epoch:6, step:20, loss:0.4046095907688141\n",
      "epoch:6, step:30, loss:0.40109163522720337\n",
      "epoch:6, step:40, loss:0.38858869671821594\n",
      "epoch:6, step:50, loss:0.41756463050842285\n",
      "epoch:6, step:60, loss:0.46486639976501465\n",
      "epoch:6, step:70, loss:0.41850075125694275\n",
      "epoch:6, step:80, loss:0.39176854491233826\n",
      "epoch:6, step:90, loss:0.3868580758571625\n",
      "epoch:6, step:100, loss:0.43457213044166565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       155\n",
      "           1       0.91      0.89      0.90       345\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.84      0.85      0.85       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.866\n",
      "AUC: 0.9268069191210846\n",
      "saved model:  ./model/bert_dnn_6.model\n",
      "epoch:7, step:10, loss:0.4059884250164032\n",
      "epoch:7, step:20, loss:0.40888792276382446\n",
      "epoch:7, step:30, loss:0.39916667342185974\n",
      "epoch:7, step:40, loss:0.4073812663555145\n",
      "epoch:7, step:50, loss:0.4053972363471985\n",
      "epoch:7, step:60, loss:0.4051210582256317\n",
      "epoch:7, step:70, loss:0.41913318634033203\n",
      "epoch:7, step:80, loss:0.4051087498664856\n",
      "epoch:7, step:90, loss:0.41990962624549866\n",
      "epoch:7, step:100, loss:0.39672717452049255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       155\n",
      "           1       0.91      0.89      0.90       345\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.84      0.85      0.84       500\n",
      "weighted avg       0.87      0.86      0.86       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.864\n",
      "AUC: 0.9280598410472184\n",
      "saved model:  ./model/bert_dnn_7.model\n",
      "epoch:8, step:10, loss:0.3849932551383972\n",
      "epoch:8, step:20, loss:0.4146333634853363\n",
      "epoch:8, step:30, loss:0.40953317284584045\n",
      "epoch:8, step:40, loss:0.398123174905777\n",
      "epoch:8, step:50, loss:0.39869821071624756\n",
      "epoch:8, step:60, loss:0.38304370641708374\n",
      "epoch:8, step:70, loss:0.4102857708930969\n",
      "epoch:8, step:80, loss:0.4156968295574188\n",
      "epoch:8, step:90, loss:0.3973374366760254\n",
      "epoch:8, step:100, loss:0.42341551184654236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78       155\n",
      "           1       0.91      0.87      0.89       345\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.83      0.85      0.84       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.856\n",
      "AUC: 0.9291257597007947\n",
      "saved model:  ./model/bert_dnn_8.model\n",
      "epoch:9, step:10, loss:0.37348756194114685\n",
      "epoch:9, step:20, loss:0.421467125415802\n",
      "epoch:9, step:30, loss:0.3696461319923401\n",
      "epoch:9, step:40, loss:0.4180992543697357\n",
      "epoch:9, step:50, loss:0.41242972016334534\n",
      "epoch:9, step:60, loss:0.40393567085266113\n",
      "epoch:9, step:70, loss:0.3996097147464752\n",
      "epoch:9, step:80, loss:0.3827632665634155\n",
      "epoch:9, step:90, loss:0.4397268295288086\n",
      "epoch:9, step:100, loss:0.3957260549068451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78       155\n",
      "           1       0.92      0.88      0.89       345\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.83      0.85      0.84       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.858\n",
      "AUC: 0.929256661991585\n",
      "saved model:  ./model/bert_dnn_9.model\n",
      "epoch:10, step:10, loss:0.38457533717155457\n",
      "epoch:10, step:20, loss:0.4068971276283264\n",
      "epoch:10, step:30, loss:0.40848761796951294\n",
      "epoch:10, step:40, loss:0.4041728675365448\n",
      "epoch:10, step:50, loss:0.4117070138454437\n",
      "epoch:10, step:60, loss:0.38697195053100586\n",
      "epoch:10, step:70, loss:0.4062800109386444\n",
      "epoch:10, step:80, loss:0.3946671485900879\n",
      "epoch:10, step:90, loss:0.40975210070610046\n",
      "epoch:10, step:100, loss:0.38564714789390564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.79       155\n",
      "           1       0.92      0.88      0.90       345\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.84      0.85      0.84       500\n",
      "weighted avg       0.87      0.86      0.86       500\n",
      "\n",
      "å‡†ç¡®ç‡: 0.862\n",
      "AUC: 0.9301168770453484\n",
      "saved model:  ./model/bert_dnn_10.model\n"
     ]
    }
   ],
   "source": [
    "# è¿­ä»£è®­ç»ƒ\n",
    "for epoch in range(num_epoches):\n",
    "    total_loss = 0\n",
    "    for i, (words, labels) in enumerate(train_loader):\n",
    "        tokens = tokenizer(words, padding=True)\n",
    "        input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "            bert_output = last_hidden_states[0][:, 0]\n",
    "        optimizer.zero_grad()               # æ¢¯åº¦æ¸…é›¶\n",
    "        outputs = net(bert_output)          # å‰å‘ä¼ æ’­\n",
    "        logits = outputs.view(-1)           # å°†è¾“å‡ºå±•å¹³\n",
    "        loss = criterion(logits, labels)    # lossè®¡ç®—\n",
    "        total_loss += loss\n",
    "        loss.backward()                     # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦\n",
    "        optimizer.step()                    # æ¢¯åº¦æ›´æ–°\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"epoch:{}, step:{}, loss:{}\".format(epoch+1, i+1, total_loss/10))\n",
    "            total_loss = 0\n",
    "    \n",
    "    # learning_rate decay\n",
    "    scheduler.step()\n",
    "    \n",
    "    # test\n",
    "    test()\n",
    "    \n",
    "    # save model\n",
    "    model_path = \"./model/bert_dnn_{}.model\".format(epoch+1)\n",
    "    torch.save(net, model_path)\n",
    "    print(\"saved model: \", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496e25e",
   "metadata": {
    "cell_id": 23
   },
   "source": [
    "### æ‰‹åŠ¨è¾“å…¥å¥å­ï¼Œåˆ¤æ–­æƒ…æ„Ÿå€¾å‘ï¼ˆ1æ­£/0è´Ÿï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0aeda8",
   "metadata": {
    "cell_id": 38
   },
   "outputs": [],
   "source": [
    "net = torch.load(\"./model/bert_dnn_8.model\")    # è®­ç»ƒè¿‡ç¨‹ä¸­çš„å·…å³°æ—¶åˆ»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d401769c",
   "metadata": {
    "cell_id": 37
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8641],\n",
       "        [0.1763]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\"åä¸½ç¹è£çš„åŸå¸‚ã€å……æ»¡å›å¿†çš„å°é•‡ã€éƒéƒè‘±è‘±çš„å±±è°·...\", \"çªç„¶å°±è§‰å¾—äººé—´ä¸å€¼å¾—\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0][:, 0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a561f7dc",
   "metadata": {
    "cell_id": 27,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9646],\n",
       "        [0.9848]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\"ä»Šå¤©å¤©æ°”çœŸå¥½\", \"ä»Šå¤©å¤©æ°”ç‰¹åˆ«ç‰¹åˆ«æ£’\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0][:, 0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d172b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2533],\n",
       "        [0.6691]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\"å¤–é¢åœ¨ä¸‹é›¨\", \"æˆ‘æƒ³åƒç‰›è‚‰\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0][:, 0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b187c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "max_cell_id": 45,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
